{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoms = ['li','be','n','ne','na','mg','p','ar','k','ca','zn','as','kr','rb','sr','cd','sb','xe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108567 3\n",
      "86028 4\n",
      "59757 7\n",
      "46683 10\n",
      "114353 11\n",
      "98148 12\n",
      "79005 15\n",
      "63195 18\n",
      "130898 19\n",
      "115705 20\n",
      "86324 30\n",
      "83240 33\n",
      "70946 36\n",
      "136914 37\n",
      "122591 38\n",
      "93142 48\n",
      "90966 51\n",
      "79378 54\n",
      "1665840\n"
     ]
    }
   ],
   "source": [
    "tdat = 0\n",
    "ne = []\n",
    "ri = []\n",
    "rho = []\n",
    "KED = []\n",
    "pop = []\n",
    "gradrho = []\n",
    "laprho = []\n",
    "gradrhogradgradrhogradrho = []\n",
    "gradrhogradlaprho = []\n",
    "lap2rho = []\n",
    "gradgradrho2 = []\n",
    "modgradrhogradgradrho2 = []\n",
    "modgradlaprho2 = []\n",
    "gradlaprhogradgradrhogradrho = []\n",
    "for atom in atoms: \n",
    "    file = str('/home/lrincon/dl_ked/pinn/data_pinn/'+atom+'.dat') \n",
    "    f = open(file,'r') \n",
    "    linea = f.readline() \n",
    "    fields = linea.split() \n",
    "    ndat = int(fields[0]) \n",
    "    nel = int(fields[1]) \n",
    "    tdat = tdat + ndat\n",
    "    print(ndat,nel) \n",
    "    for i in range(ndat): \n",
    "        linea = f.readline() \n",
    "        fields = linea.split()\n",
    "        ne.append(nel)\n",
    "        ri.append(float(fields[0])) \n",
    "        rho.append(float(fields[1]))\n",
    "        KED.append(float(fields[2]))  \n",
    "        pop.append(float(fields[3]))\n",
    "        gradrho.append(float(fields[4])) \n",
    "        laprho.append(float(fields[5]))\n",
    "        gradrhogradgradrhogradrho.append(float(fields[6]))\n",
    "        gradrhogradlaprho.append(float(fields[7]))\n",
    "        lap2rho.append(float(fields[8]))\n",
    "        gradgradrho2.append(float(fields[9]))\n",
    "        modgradrhogradgradrho2.append(float(fields[10]))\n",
    "        modgradlaprho2.append(float(fields[11]))\n",
    "        gradlaprhogradgradrhogradrho.append(float(fields[12]))\n",
    "    f.close()\n",
    "print(tdat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.ndarray(shape=(tdat,14))\n",
    "zeta = 2.0*np.power(3.0*np.power(np.pi,2),1.0/3.0)\n",
    "for i in range(tdat):\n",
    "    data[i][0] = KED[i]\n",
    "    data[i][1] = pop[i]\n",
    "    data[i][2] = ri[i]\n",
    "    data[i][3] = ne[i]\n",
    "    data[i][4] = rho[i]\n",
    "    z = gradrho[i]/(zeta*np.power(rho[i],4.0/3.0))\n",
    "    data[i][5] = np.power(z,2)\n",
    "    z = laprho[i]/(np.power(zeta,2)*np.power(rho[i],5.0/3.0))\n",
    "    data[i][6] = z\n",
    "    z = gradrhogradgradrhogradrho[i]/(np.power(zeta,4)*np.power(rho[i],13.0/3.0))\n",
    "    data[i][7] = z\n",
    "    z = gradrhogradlaprho[i]/(np.power(zeta,4)*np.power(rho[i],10.0/3.0))\n",
    "    data[i][8] = z\n",
    "    z = lap2rho[i]/(np.power(zeta,4)*np.power(rho[i],7.0/3.0))\n",
    "    data[i][9] = z\n",
    "    z = gradgradrho2[i]/(np.power(zeta,4)*np.power(rho[i],10.0/3.0))\n",
    "    data[i][10] = z\n",
    "    z = modgradrhogradgradrho2[i]/(np.power(zeta,6)*np.power(rho[i],6))\n",
    "    data[i][11] = z\n",
    "    z = modgradlaprho2[i]/(np.power(zeta,6)*np.power(rho[i],4))\n",
    "    data[i][12] = z\n",
    "    z = gradlaprhogradgradrhogradrho[i]/(np.power(zeta,6)*np.power(rho[i],5))\n",
    "    data[i][13] = z\n",
    "np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization function: f_norm\n",
    "def f_norm(x):\n",
    "    return x/(1.0+x)\n",
    "# inverse\n",
    "def fi_norm(x):\n",
    "    return x/(1.0-x)\n",
    "def df_norm(x):\n",
    "    return 1.0/np.power(1.0+x,2)\n",
    "def dfi_norm(x):\n",
    "    return 1.0/np.power(1.0-x,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16658\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "fraction = 0.010\n",
    "dim = np.int(fraction*tdat)\n",
    "print(dim)\n",
    "# Target data: 0: ked, 1: potential\n",
    "ked_target = np.ndarray(shape=(dim,2))\n",
    "# Descriptors: 0: number of electron, 1: reduced gradient, 2: reduced laplacian\n",
    "ked_input = np.ndarray(shape=(dim,3))\n",
    "for i in range(dim):\n",
    "    KEDi = data[i][0]\n",
    "    popi = data[i][1]\n",
    "    nei = data[i][3]\n",
    "    rhoi = data[i][4]\n",
    "    s2 = data[i][5]\n",
    "    p = data[i][6]\n",
    "    CTF = (3.0/10.0)*np.power(3.0*np.power(np.pi,2),2.0/3.0)\n",
    "    TF = CTF*np.power(rhoi,5.0/3.0)\n",
    "    vW = (5.0/3.0)*TF*s2\n",
    "    delta = (KEDi-TF-(1.0/9.0)*vW)/TF\n",
    "    ked_target[i][0] = delta\n",
    "    ked_target[i][1] = (popi/(CTF*np.power(rhoi,2.0/3.0)))-(5.0/3.0)-(5.0/27.0)*s2+(10.0/27.0)*p\n",
    "    ked_input[i][0] = nei/100.0\n",
    "    ked_input[i][1] = s2\n",
    "    ked_input[i][2] = p\n",
    "ked_target = tf.convert_to_tensor(ked_target,dtype='float32')\n",
    "ked_input = tf.convert_to_tensor(ked_input,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.ndarray(dim)\n",
    "a2 = np.ndarray(dim)\n",
    "a3 = np.ndarray(dim)\n",
    "a4 = np.ndarray(dim)\n",
    "a5 = np.ndarray(dim)\n",
    "a6 = np.ndarray(dim)\n",
    "a7 = np.ndarray(dim)\n",
    "a8 = np.ndarray(dim)\n",
    "a9 = np.ndarray(dim)\n",
    "for i in range(dim):\n",
    "    s2 = data[i][5]\n",
    "    p  = data[i][6]\n",
    "    q0 = data[i][7]\n",
    "    q1 = data[i][8]\n",
    "    q2 = data[i][9]\n",
    "    q3 = data[i][10]\n",
    "    h0 = data[i][11]\n",
    "    h1 = data[i][12]\n",
    "    h2 = data[i][13]\n",
    "    a1[i] = 5.0/3.0\n",
    "    a2[i] = -((2.0/3.0)*s2+2.0*p)\n",
    "    a3[i] = -(5.0/3.0)*p\n",
    "    a4[i] = (16.0/3.0)*np.power(s2,2)-4.0*q0\n",
    "    a5[i] = (88.0/9.0)*np.power(s2,2)+(2.0/3.0)*s2*p-(32.0/3.0)*q0+2.0*q3\n",
    "    a6[i] = (40.0/9.0)*s2*p-(5.0/3.0)*np.power(p,2)-(10.0/3.0)*q1+q2\n",
    "    a7[i] = (64.0/9.0)*np.power(s2,3)-(32.0/3.0)*s2*q0+4.0*h0\n",
    "    a8[i] = (80.0/9.0)*np.power(s2,2)*p-(16.0/3.0)*s2*q1-(20.0/3.0)*p*q0+4.0*h2\n",
    "    a9[i] = (25.0/9.0)*s2*np.power(p,2)-(10.0/3.0)*p*q1+h1\n",
    "a1 = tf.convert_to_tensor(a1,dtype='float32')\n",
    "a2 = tf.convert_to_tensor(a2,dtype='float32')\n",
    "a3 = tf.convert_to_tensor(a3,dtype='float32')\n",
    "a4 = tf.convert_to_tensor(a4,dtype='float32')\n",
    "a5 = tf.convert_to_tensor(a5,dtype='float32')\n",
    "a6 = tf.convert_to_tensor(a6,dtype='float32')\n",
    "a7 = tf.convert_to_tensor(a7,dtype='float32')\n",
    "a8 = tf.convert_to_tensor(a8,dtype='float32')\n",
    "a9 = tf.convert_to_tensor(a9,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define residual of the potential\n",
    "def pop_res(u, du_ds, du_dp, d2u_ds2, d2u_dsp, d2u_dp2, d3u_ds2p, d3u_dsp2, d3u_dp3, a1, a2, a3, a4, a5, a6, a7, a8, a9):\n",
    "    product1 = tf.math.multiply(a1,u)\n",
    "    product2 = tf.math.multiply(a2,du_ds)\n",
    "    product3 = tf.math.multiply(a3,du_dp)\n",
    "    product4 = tf.math.multiply(a4,d2u_ds2)\n",
    "    product5 = tf.math.multiply(a5,d2u_dsp)\n",
    "    product6 = tf.math.multiply(a6,d2u_dp2)\n",
    "    product7 = tf.math.multiply(a7,d3u_ds2p)\n",
    "    product8 = tf.math.multiply(a8,d3u_dsp2)\n",
    "    product9 = tf.math.multiply(a9,d3u_dp3)\n",
    "    return product1 + product2 + product3 + product4 + product5 + product6 + product7 + product8 + product9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KED_model(input_shape = (3)):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Dense(50,activation='relu')(inputs)\n",
    "    x = Dense(50,activation='relu')(x)\n",
    "    x = Dense(50,activation='relu')(x)\n",
    "    x = Dense(50,activation='relu')(x)\n",
    "    x = Concatenate()([inputs, x])\n",
    "    x = Dense(25,activation='relu')(x)\n",
    "    x = Dense(1,activation='relu')(x)\n",
    "    KED = tf.keras.Model(inputs=inputs, outputs= x)    \n",
    "    return KED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 50)           200         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 50)           2550        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 50)           2550        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 50)           2550        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 53)           0           input_1[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 25)           1350        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            26          dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 9,226\n",
      "Trainable params: 9,226\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = KED_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine gradient\n",
    "def get_r(model,ked_input,a1,a2,a3,a4,a5,a6,a7,a8,a9):\n",
    "    \n",
    "    # A tf.GradientTape is used to compute derivatives in TensorFlow\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Split x_train to compute partial derivatives\n",
    "        ne_norm,s2_norm, p_norm = ked_input[:,0:1], ked_input[:,1:2], ked_input[:,2:3]\n",
    "\n",
    "        # Variables s_norm and p_norm are watched during tape\n",
    "        # to compute derivatives du_ds and du_dp\n",
    "        tape.watch(ne_norm)\n",
    "        tape.watch(s2_norm)\n",
    "        tape.watch(p_norm)\n",
    "\n",
    "        # Determine residual \n",
    "        u = model(tf.stack([ne_norm[:,0],s2_norm[:,0],p_norm[:,0]],axis=1))\n",
    "\n",
    "        # Compute gradient du_dx and du_dp within the GradientTape\n",
    "        # since we need second derivatives\n",
    "        du_ds = tape.gradient(u, s2_norm)\n",
    "        du_dp = tape.gradient(u, p_norm)    \n",
    "        d2u_ds2 = tape.gradient(du_ds, s2_norm)\n",
    "        d2u_dsp = tape.gradient(du_ds, p_norm)\n",
    "        d2u_dp2 = tape.gradient(du_dp, p_norm)\n",
    "    # Compute the second derivative\n",
    "    d3u_ds2p = tape.gradient(d2u_ds2, p_norm)\n",
    "    d3u_dsp2 = tape.gradient(d2u_dsp, p_norm)\n",
    "    d3u_dp3  = tape.gradient(d2u_dp2, p_norm)\n",
    "\n",
    "    del tape\n",
    "    \n",
    "    # Compute residual\n",
    "    return pop_res(u, du_ds, du_dp, d2u_ds2, d2u_dsp, d2u_dp2, d3u_ds2p, d3u_dsp2, d3u_dp3, a1, a2, a3, a4, a5, a6, a7, a8, a9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss\n",
    "def compute_loss(model,ked_input,ked_target,a1,a2,a3,a4,a5,a6,a7,a8,a9):\n",
    "    \n",
    "    # Compute the residual\n",
    "    r = get_r(model,ked_input,a1,a2,a3,a4,a5,a6,a7,a8,a9)\n",
    "    phi_r = tf.reduce_mean(tf.square(r-ked_target[:,1]))\n",
    "    \n",
    "    # Initialize loss with the residual\n",
    "    loss = phi_r\n",
    "    \n",
    "    # Add the error in KED to the loss\n",
    "    for i in range(len(ked_input)):\n",
    "        x_input = tf.expand_dims(ked_input[i],axis=0)\n",
    "        u_pred = model(x_input)\n",
    "        loss += tf.reduce_mean(tf.square(u_pred-ked_target[i,0]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Neural Network trainable variables gradient\n",
    "def get_grad(model,ked_input,ked_target,a1,a2,a3,a4,a5,a6,a7,a8,a9):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # This tape is for derivatives with\n",
    "        # respect to trainable variables\n",
    "        tape.watch(model.trainable_variables)\n",
    "        loss = compute_loss(model,ked_input,ked_target,a1,a2,a3,a4,a5,a6,a7,a8,a9)\n",
    "\n",
    "    g = tape.gradient(loss,model.trainable_variables)\n",
    "    del tape\n",
    "\n",
    "    return loss, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up optimizer\n",
    "# Initialize model\n",
    "model = KED_model()\n",
    "\n",
    "# We choose a piecewise decay of the learning rate, i.e., the\n",
    "# step size in the gradient descent type algorithm\n",
    "# the first 1000 steps use a learning rate of 0.01\n",
    "# from 1000 - 3000: learning rate = 0.001\n",
    "# from 3000 onwards: learning rate = 0.0005\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
    "\n",
    "# Choose the optimizer\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# Define one training step as a TensorFlow function to increase speed of training\n",
    "@tf.function\n",
    "def train_step():\n",
    "    # Compute current loss and gradient w.r.t. parameters\n",
    "    loss, grad_theta = get_grad(model,ked_input,ked_target,a1,a2,a3,a4,a5,a6,a7,a8,a9)\n",
    "    \n",
    "    # Perform gradient descent step\n",
    "    optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Number of training epochs\n",
    "N = 5000\n",
    "hist = []\n",
    "\n",
    "for i in range(N+1):\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    loss = train_step()\n",
    "    \n",
    "    # Append current loss to hist\n",
    "    hist.append(loss.numpy())\n",
    "    \n",
    "    # print the loss\n",
    "    print('Iteration {:06d} : (loss = {:10.8e} , time = {} )'.format(i,loss,time()-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
